# -*- coding: utf-8 -*-
"""Copy of Ex_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y_vQ7yq21jMVemAbphJP1NLPxiw9I6bI

**IMPORT** LIBRARIES

READ THE DATA:

- In this challenge you will use the dataset Titanic disaster.
- With the following lines of code, you can import the dataset form the file "train.csv", extract te column of labels (column "Survived") and read the infos of the other features.

you can also follow kaggle competition:
https://www.kaggle.com/code/amitkumarjaiswal/beginner-s-tutorial-to-titanic-using-scikit-learn/notebook

---
"""

import numpy as np 
import pandas as pd 
from google.colab import drive
#if you downloaded the file in the Colab directory in your Google Drive, use the following lines of code
drive.mount('/gdrive', force_remount=True)
Tr_file="/gdrive/My Drive/Colab Notebooks/T_db/train.csv" 
dataset=pd.read_csv(Tr_file)
X_tr=dataset.drop('Survived',axis=1)
Y_tr=dataset.Survived

X_tr.info()
Y_tr
print(X_tr.isnull().sum(axis=0))

"""FEATURES ENGINEERING:

- Evaluate if there are features with missing values and modify them (see https://scikit-learn.org/stable/modules/impute.html)
- Find meaningful features (some features are not needed because do not contain useful infos)
- Encode categorical features with numerical values (see https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features)

**MISSING VALUES**
"""

# REPLACEMENT WITH  FIXED VALUES 
X_tr["Embarked"].fillna("S", inplace =True);

#X_tr.dropna(axis=1, how= 'any')
#X_tr.dropna() #  delete rows with missing data 

print(X_tr.isnull().sum(axis=0))

# REPLACEMENT WITH MEAN VALUE OR MOST FREQUENT VALUES
from sklearn.impute import SimpleImputer
#Age
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp = imp.fit(X_tr[["Age"]])
X_tr["Age"] = imp.transform(X_tr[["Age"]])
#print(X_tr[["Age"]]) 

imp1 = SimpleImputer(strategy="most_frequent")
imp1 = imp1.fit(X_tr[["Cabin"]])
X_tr["Cabin"] = imp1.transform(X_tr[["Cabin"]])
#print(features[["Cabin"]])
print(X_tr.isnull().sum(axis=0))
X_tr

"""**CATEGORICAL FEATURES**"""

from sklearn import preprocessing
enc = preprocessing.OrdinalEncoder()
enc = enc.fit(X_tr[['Sex']])
X_tr['Sex'] = enc.transform(X_tr[['Sex']])
enc = enc.fit(X_tr[['Name']])
X_tr['Name'] = enc.transform(X_tr[['Name']])
enc = enc.fit(X_tr[['Embarked']])
X_tr['Embarked'] = enc.transform(X_tr[['Embarked']])
enc = enc.fit(X_tr[['Ticket']])
X_tr['Ticket'] = enc.transform(X_tr[['Ticket']])
enc = enc.fit(X_tr[['Cabin']])
X_tr['Cabin'] = enc.transform(X_tr[['Cabin']])
print(X_tr.shape)
X_tr

"""#MEANINGFUL FEATURES"""

#Normalization
from sklearn import preprocessing

scaler = preprocessing.MinMaxScaler().fit(X_tr) 

X_tr = scaler.transform(X_tr)
X_tr = preprocessing.normalize(X_tr, norm='l2')

X_tr

"""#FEATURE SELECTION

"""

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

test = SelectKBest(score_func=chi2, k=8)
fit = test.fit(X_tr, Y_tr)

X_tr_sel = test.fit_transform(X_tr , Y_tr)

#Reduction
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis(n_components=1)
lda.fit(X_tr, Y_tr)
X_tr_r = lda.transform(X_tr)
print(X_tr_r.shape)

#use the 3-knn to verified if your choice are good
from sklearn.neighbors import KNeighborsClassifier     #KNN
from sklearn.metrics import accuracy_score
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_tr_r,Y_tr)
scores = knn.predict(X_tr_r)

# Show prediction accuracy
print('\nPrediction accuracy:')
print('{:.2%}\n'.format(accuracy_score(Y_tr, scores)))

"""TRAIN AND TEST DIFFERENT CLASSIFIERS:

- Find the best parameters for each classifier with a stratified 10-fold cross validation (see https://scikit-learn.org/stable/modules/cross_validation.html)

- Try these models:

>- Naive Bayes (see https://scikit-learn.org/stable/modules/naive_bayes.html) 
>- kNN (see https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification)
>- Decision tree (see https://scikit-learn.org/stable/modules/tree.html)
>- Logistic regression (see https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)

- Show the final performances in terms of accuracy
"""

#Write your code here for applying cross validation to each model and optimize the classifiers' parameters
#10-fold cross validation

# code for applying different cross validation to each model
################################################################################
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import LeavePOut
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate

# kf = KFold(n_splits=10)
# rkf = RepeatedKFold(n_splits=10, n_repeats=2, random_state=42)
# loo = LeaveOneOut() # no argument is required
# lpo = LeavePOut(p=10) # requires very long computational time
ss = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)
# skf = StratifiedKFold(n_splits=10)
################################################################################


#classifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import tree
from sklearn.linear_model import LogisticRegression

gnb = GaussianNB()
knn = KNeighborsClassifier(n_neighbors=3)
tree = tree.DecisionTreeClassifier()
log_reg = LogisticRegression(random_state=0)

X = np.array(X_tr)
y = np.array(Y_tr)

cross_gnb     = cross_val_score(gnb,     X, y, cv=ss)
cross_knn     = cross_val_score(knn,     X, y, cv=ss)
cross_tree    = cross_val_score(tree,    X, y, cv=ss)
cross_log_reg = cross_val_score(log_reg, X, y, cv=ss)

print("Bayes Naive accuracy:", cross_gnb.mean())
print("KNN accuracy:", cross_knn.mean())
print("Decision Tree accuracy:", cross_tree.mean())
print("Logistic Regression accuracy:", cross_log_reg.mean())

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X_tr, Y_tr, test_size=0.15, random_state=0)
from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
y_pred_naive_bayes = gnb.fit(X_train, y_train).predict(X_test)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)

knn.fit(X_train,y_train)
y_pred_knn = knn.predict(X_test)

from sklearn import tree

tree = tree.DecisionTreeClassifier()

tree = tree.fit(X_train,y_train)
y_pred_tree = tree.predict(X_test)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(random_state=0).fit(X_train, y_train)

y_pred_log_reg = log_reg.predict(X_test)


print('Naive Bayes:', accuracy_score(y_test, y_pred_naive_bayes))
print('KNN:', accuracy_score(y_test, y_pred_knn))
print('Decision Tree:', accuracy_score(y_test, y_pred_tree))
print('Logistic Regression:', accuracy_score(y_test, y_pred_log_reg))

